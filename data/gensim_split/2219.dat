As
a
Big
Data
Architect
in
the
Aero
Services
organization
you
will
be
responsible
for
defining
the
Aerospace-wide
big_data
platform
strategy
that
meets
the
needs
of
our
data_science
teams
and
customer-facing_web
and
mobile_applications
This
person
will
work_closely
with
product_management
and
engineering_teams
to
understand
the
customer
requirements
needs
aircraft_systems
capabilities
and
available
disparate
data
pipes
to
determine
the
optimal_approach
to
managing
load
store
access
and
govern
the
data
in
our
Big
Data
platform
Key
to
the
growth
of
the
Aero
Services
Org
will
be
our
ability
to
monetize_critical
data
that
is
produced
by
our
suite
of
Aerospace
products
Main
Responsibilities
Understand
and
translate
project
requirements
into
technical
requirements
and
solutions
for
data
engineering
team
to
execute
Architect
and
design
a
Big
Data
analytics
platform
that
is
scalable
optimized
and
fault-tolerant
Perform
program
reviews
to
ensure
that
data
design
elements
are
reusable
and
repeatable
across
projects
Define
and
develop
guidelines_standards
and
processes
to
ensure
the
highest
data
quality
and
integrity
in
the
data_stores
residing
on
the
data_lake
Participate
in
setting_strategy
and
standards
through
data
architecture
and
implementation
leveraging
big_data
and
analytics
tools
and
technologies
Work
closely
with
data_scientists
and
product_managers
to
understand
their
data
requirements
for
existing
and
future
projects
on
data
analytics
applications
Work
with
IT
and
data
owners
to
understand
the
types
of
data_collected
in
various
databases
and
data_warehouses
Research
and
suggest
new
toolsets
methods
to
improve
data_ingestion
storage
and
data
access
in
the
analytics
platform
Provide
guidance
mentor
data
engineering
team
Possess
hands-on_technical
experience
in
big_data
technologies
Keen
business_acumen
to
recognize
and
recommend_cost-effective
and
scalable
platform
solutions
that
best_meet
current
and
future
business_needs
Ability
to
execute_projects
using
an
agile
approach
in
a
multi-disciplinary
matrixed_environment
Comfortable
working
in
a
dynamic
research
and
development
environment
with
several_ongoing
concurrent_projects
Enjoys
exploring
and
learning
new_technologies
You
Must
Have
Bachelor
Masters
or
PhD
degree
in
computer_science
IT
engineering
or
other
relevant_field
with
a
10_years
of
data
management
experience
Minimum
of
5_years
of
hands-on_experience
in
designing_deploying
and
supporting
enterprise
data_warehouses
and
distributed
data
processing
platforms
Minimum
of
5_years
of
ETL
ELT
experience
in
traditional
data
MSSQL
MySQL
Oracle
etc
and
Big
data
platform
using
Hadoop
ecosystem
Spark
Hive
Pig
Sqoop
Flume
etc
Minimum
of
3_years
of
experience
in
scripting_languages
Perl
Python
Java
etc
Minimum
of
3_years
of
experience
in
NoSQL
solutions
Hbase
Cassandra
MongoDB
CouchDB
etc
and
managing
unstructured_data
Must
be
a
US
Citizen
due
to
contractual
requirements
We
Value
Certification
in
Hadoop
and
other
big_data
tools
and
technologies
Working
experience
in
IOT
projects
Experience
with
data
management
on
public_cloud
hosting_services
Deep
knowledge
in
data_mining
machine_learning
natural_language
processing
or
information_retrieval
Experience
with
Agile
software_development
methodology
Ability
to
work
in
a
fast-paced
and
ambiguous_environment
Location
Phoenix
AZ