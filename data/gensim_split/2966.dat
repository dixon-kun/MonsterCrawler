CLIENT
OVERVIEW
Our
client
is
utilizing
cutting_edge
open_source
technologies
and
frameworks
to
derive_actionable
insights
from
massive_amounts
of
data
Our
solution
provides
capability
to
ingest
petabytes
of
structured
and
unstructured_data
and
build
analytic_pipelines
that
crunch
through
this
data
JOB
OVERVIEW
We
are
looking
for
a
Big
Data
Engineer
that
will
utilize
our
base
platform
to
develop
ingestion
and
analytic_pipelines
utilizing
a
multitude
of
open_source
technologies
The
primary_focus
will
be
on
implementing
optimal
solutions
utilizing
best_practices
that
can
be
shared
across
all
our
clients
This
position_requires
working
with
technical
leads
data_scientists
and
project_managers
in
a
Scrum
based
Agile
environment
JOB
RESPONSIBILITIES
Create
configure
implement
document
and
maintain
ingestion
enrichment
and
analytic_pipelines
using
distributed
big_data
platform
Extend
base
platform
functionality
by
adding
new
ingestion
and
analytic
sources
Identify
evaluate
and
implement
big_data
tools
and
frameworks
required
to
provide
requested_capabilities
JOB
EXPERIENCE
REQUIRED
Java
development
experience
Scripting
language
experience
Perl
Python
JavaScript
Proficient
understanding
of
distributed_computing
principles
Proficiency
with
Hadoop
v2
MapReduce
HDFS
Experience
with
building_stream-processing
systems
using
solutions
such
as
Storm
or
Spark
Experience
with
integration
of
data
from
multiple
data_sources
Experience
with
NoSQL
databases
such
as
Elasticsearch
MongoDB
Cassandra
Knowledge
of
various
ETL
techniques
and
frameworks
such
as
Flume
Logstash
Experience
with
various_messaging
systems
such
as
Kafka
or
RabbitMQ
Experience
with
Cloudera
Hortonworks
distributions
Bachelor's
degree
or
equivalent
professional
experience
JOB
EXPERIENCE
DESIRED
Good
knowledge
of
Big
Data
querying
tools
such
as
Pig
Hive
and
Impala
Management
of
Hadoop
cluster
with
all
included
services
Ability
to
solve
any
ongoing
issues
with
operating
the
cluster