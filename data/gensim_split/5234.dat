Role
purpose
The
Elsevier
Optimized
Learning
Suite
EOLS
is
an
Adaptive
Learning
project
with
the
objective
of
improving
the
experience
of
learning
by
recommending
customized
content
for
its
users
through
its
usage
of
near_real
time
streaming
analytics
The
EOLS
Analytics
platform
utilizes
a
number
of
cutting_edge
technologies
in
the
Hadoop
ecosystem
to
perform
stream_processing
batch
analytics
machine_learning
in
the
cloud
These
services
developed
will
feed
crucial
business_intelligence
to
our
education
data
driven
decision-making
from
both
an
institution
and
classroom
perspective
and
provide
analytical
dashboards
for
both
instructors
and
students
with
live
data
As
a
Senior
Hadoop
Engineer
Architect
for
the
EOLS
Analytics
platform
you
will
have
the
following
accountabilities
Lead
a
team
of
highly_motivated
data
integration
engineers
Provide
technical
advisory
and
expertise
on
Analytics
subject_matter
Create
implement
and
execute
the
roadmap
for
providing
Analytics
insight
and
Machine
Learning
Identify
useful
technology
that
can
be
used
to
fulfill
user
story
requirements
from
an
Analytics
perspective
Experiment
with
new
technology
as
an
ongoing
proof
of
concept
Architect
and
develop
data
integration
pipelines_using
a
combination
of
stream
and
batch_processing
techniques
Integrate
multiple
data_sources
using
Extraction
Transformation
and
Loading
ETL
Build
data_lake
and
data_marts
using
HDFS
NoSQL
and
Relational
databases
Manage
multiple
Big
Data
clusters
and
data
storage
in
the
cloud
Collect
and
process
event
data
from
multiple
application
sources
with
both
internal
Elsevier
and
external
vendor_products
Understand
data_science
and
work_directly
with
data_scientists
and
machine_learning
engineers
Experience
Skills
Qualifications
required
8_years
experience
in
software
programming
using
Java
JavaScript
Spring
SQL
etc
3_years
experience
in
service
integration
using
REST
SOAP
RPC
etc
3_years
experience
in
Data
Management
Data
Modeling
Python
Scala
or
any
semi-functional
programming
preferred
Excellent
SQL
skills
from
different
range
levels
of
ANSI
compliancy
Basic
knowledge
of
Applied
Statistics
Advanced
knowledge
of
Systems
and
Service
Architecture
Advanced
knowledge
of
Polyglot
Persistence
and
use
of
RDBMS
In-Memory
Key
Value
stores
BigTable
databases
and
Distributed
File
Systems
such
as
HDFS
and
Amazon
S3
Industry
experience_working
with
large_scale
stream_processing
batch_processing
and
data_mining
Extensive
knowledge
of
the
Hadoop
ecosystem
and
its
components
such
as
HDFS
Kafka
Spark
Flume
Oozie
HBase
Hive
Experience
with
at
least_one
of
the
Hadoop
distributions
such
as
Cloudera
Hortonworks
MapR
or
Pivotal
Experience
with
Cloud
services
such
as
AWS
or
Azure
Experience
with
Linux
UNIX
systems
and
the
best_practices
for
deploying_applications
to
Hadoop
from
those
environments
Advanced
knowledge
of
ETL
Data
Routing
and
understanding
of
tools
such
as
NiFi
Kinesis
etc
Good
understanding
of
DevOps
SDLC
and
Agile
methodology
Software
Infrastructure
Diagrams
such
as
Sequence
UML
Data
Flows
Requirements
Analysis
Planning
Problem
Solving
Strategic
Planning
Excellent
Verbal
Communication
Self-Motivated
with
Initiative
Education
business
domain_knowledge
preferred