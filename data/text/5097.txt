('and', 32)('in', 18)('to', 17)('of', 16)('with', 13)('data', 10)('a', 10)('or', 10)('Experience', 9)('Hive', 8)('for', 8)('experience', 7)('the', 7)('-', 7)('GA', 6)('design', 6)('able', 6)('development', 6)('Atlanta', 5)('Think', 5)('systems', 5)('Must', 5)('Data', 5)('Big', 5)('will', 5)('be', 5)('team', 4)('technical', 4)('write', 4)(':&#', 4)('Senior', 4)('programming', 4)('160', 4)('Python', 4)('SQL', 4)('software', 4)('etc.', 4)('relational', 3)('Prior', 3)('from', 3)('how', 3)('efficiently', 3)('Linux', 3)('years', 3)('production', 3)('our', 3)('that', 3)('Engineer', 3)('client', 3)('distributed', 3)('partitioning', 3)('complex', 3)('database', 3)('Java', 3)('methods', 3)('non-interactive', 3)('Teradata', 3)('retrieving', 3)('Science', 3)('(batch', 3)('building', 3)('code', 2)('unstructured', 2)('skills', 2)('large', 2)('work', 2)('(', 2)('use', 2)('working', 2)('Spark', 2)('A', 2)(')', 2)('help', 2)('including', 2)('lifecycle.', 2)('related', 2)('Job', 2)('Understands', 2)('leverage', 2)('Consulting', 2)('project', 2)('data.', 2)('build', 2)('C/C++.', 2)('(e.g.', 2)('implementation', 2)('Strong', 2)('etc.)', 2)('teams', 2)('best', 2)('key', 2)('Computer', 2)('Degree', 2)('as', 2)('at', 2)('You', 2)('foreign', 2)('Understanding', 2)('Able', 2)('Abilities:', 2)('Company', 2)('Employer', 2)('scale', 2)('Responsibilities:', 2)('equivalent', 2)('leadership', 2)('by', 2)('on', 2)('etc.).', 2)('an', 2)('expertise', 2)('physical', 2)('you', 2)('and/or', 2)('Lead', 1)('OSI', 1)('configuration.', 1)('NoSQL', 1)('time.', 1)('Preferred', 1)('environment', 1)('Additionally', 1)('SWL.', 1)('Secondary', 1)('input-loaders', 1)('returning', 1)('LLC', 1)('ticket', 1)('receiving', 1)('solver', 1)('applicable', 1)('assigned.', 1)('small', 1)('HTTP', 1)('Tuning', 1)('consistent', 1)('Sales', 1)('(just', 1)('meetings.', 1)('+', 1)('designing', 1)('DSS', 1)('understanding', 1)('warehousing', 1)('mentor', 1)('suitable', 1)('followed', 1)('logical', 1)('establish', 1)('languages', 1)('Configure', 1)('6', 1)('more', 1)('&amp', 1)('company', 1)('Docker', 1)('organizations', 1)('cluster', 1)('this', 1)('clients', 1)('can', 1)('Inc', 1)('activities.', 1)('Familiarity', 1)('accept', 1)('topic', 1)('schemas', 1)('requirements', 1)('sit', 1)('provide', 1)('travel', 1)('Kerberos', 1)('machine', 1)('map', 1)('UDFs', 1)('documentation', 1)('maintain', 1)('progressively', 1)('Neurogenetics', 1)('Mentor', 1)('six', 1)("Master's", 1)('Proficient', 1)('hiring', 1)('Transactions', 1)('(file', 1)('JMS:', 1)('designs', 1)('Ambari', 1)('combination', 1)('gradle/maven', 1)('records', 1)('level', 1)('Four', 1)('API', 1)('|', 1)('Elsym', 1)('space', 1)('research', 1)('7', 1)('Kennesaw', 1)('spring-mvc', 1)('Standards-based', 1)('integrated', 1)('commits', 1)('LDAP', 1)('IT', 1)('bucketing)', 1)('REST', 1)('members', 1)('log', 1)('training', 1)('{emailaddress}', 1)('schemas.', 1)('(Streaming)', 1)('structured', 1)('Creative', 1)('documentation.', 1)('Tests', 1)('interact', 1)('co-locating', 1)('least', 1)('Automated', 1)('white', 1)('mining', 1)('pipelines', 1)('C/C++', 1)('XP', 1)('representing', 1)('to:', 1)('15', 1)('AT&amp', 1)('Serdes', 1)('experience.', 1)('Other', 1)('any', 1)('thought', 1)('efficient', 1)('responsible', 1)('high', 1)('performance', 1)('Eclaro', 1)('multiple', 1)('denomalization', 1)('Equal', 1)('topic.', 1)('sources.', 1)('Dynamic', 1)('professional', 1)('senior', 1)('Anthem', 1)('workshops', 1)('Hadoop', 1)("Bachelor's", 1)('parameters', 1)('field.', 1)('Services', 1)('Thrift', 1)('Opportunity/Affirmative', 1)('practices.', 1)('RabbitMQ', 1)('Jira', 1)('formats.', 1)('analytical', 1)('intelligence', 1)('analytics', 1)('Jenkins', 1)('spring-batch', 1)('Ability', 1)('sites', 1)('written', 1)('verbs', 1)('reading', 1)('Advanced', 1)('Full-time', 1)('Cloudera', 1)('jobs', 1)('deliver', 1)('job', 1)('Directory', 1)('modeling', 1)('languages.', 1)('installation', 1)('Primary', 1)('Nagios', 1)('Design', 1)('168780', 1)('computers.', 1)('create/update', 1)('learning', 1)('table', 1)('tuning', 1)('capable', 1)('create', 1)('50%', 1)('KDC', 1)('Active', 1)('duties', 1)('enable', 1)('R/Python', 1)('BridgeView', 1)('Action', 1)('Professional', 1)('Software', 1)('formats', 1)('backing', 1)('is', 1)('it', 1)('player', 1)('emailed', 1)('different', 1)('develop', 1)('member', 1)('authentication', 1)('veterans.', 1)('conferences.', 1)('I', 1)('Medical', 1)('differing', 1)('running', 1)('waterfall', 1)('stack', 1)('tune', 1)('lower', 1)('Education:', 1)('analysis', 1)('deliverable', 1)('academic', 1)('Manager', 1)('practices', 1)('coordinate', 1)('Engineer-', 1)('English.', 1)('States-Georgia-Atlanta', 1)('agile', 1)('4', 1)('Clojure)', 1)('calls.', 1)('read', 1)('big', 1)('Setup', 1)('Americas-United', 1)('T', 1)('background', 1)('commentary', 1)('advanced', 1)('like', 1)('Denormalization', 1)('Pig', 1)('communications', 1)('output', 1)('ActiveMQ', 1)('OR', 1)('verbal', 1)('Scala', 1)('spring', 1)('access-driven', 1)('DStream.', 1)('proper', 1)('lead', 1)('Streaming', 1)('Familiar', 1)('contributing', 1)('provider', 1)('results.', 1)('AMS', 1)('Ruby', 1)('Protobufs', 1)('business', 1)('processing', 1)('communicate', 1)('path-breaking', 1)('works)', 1)('offset', 1)('Knows', 1)('spring-hadoop', 1)('industry', 1)('Proven', 1)('Kafka', 1)('JBoss', 1)('Participate', 1)('Broader', 1)('consulting', 1)('Knowledge', 1)('scripts', 1)('functional', 1)('appropriate', 1)('Ganglia', 1)('statistics', 1)('compression', 1)('transform', 1)('long', 1)('bucketing', 1)('systems.', 1)('start', 1)('Excellent', 1)('Hive-HBase', 1)('sophisticated', 1)('link', 1)('Queries', 1)('Services/Consulting', 1)('junior', 1)('clients.', 1)('ecosystem', 1)('problem', 1)('Develop', 1)('Avro', 1)('start-up', 1)('Vagrant', 1)('single', 1)('periods', 1)('file', 1)('education', 1)('field', 1)('papers', 1)('mathematics', 1)('developers', 1)('TB', 1)('operators', 1)('Skills', 1)('scrum', 1)('includes', 1)('develops', 1)('Qualifications:', 1)('demanding', 1)('methodologies', 1)('As', 1)('time', 1)('Jobs.com', 1)    Think Big Senior Data Engineer - Teradata | Jobs.com
          Think Big Senior Data Engineer-168780
            Primary Responsibilities:
           As a Senior Data Engineer I, you will provide technical leadership to clients in a team that designs and develops path-breaking large scale cluster data processing systems. You will mentor sophisticated organizations on large scale data and analytics and work with client teams to deliver results.
           Additionally, as a senior member of our Consulting team, you will help Think Big, A Teradata Company establish thought leadership in the big data space by contributing white papers, technical commentary and representing our company at industry conferences.
            Secondary Responsibilities:
           Design and develop code, scripts and data pipelines that leverage structured and unstructured data integrated from multiple sources. Software installation and configuration. Participate in and help lead requirements and design workshops with our clients. Develop project deliverable documentation. Lead small teams of developers and coordinate development activities. Mentor junior members of the team in software development best practices. Other duties as assigned.
           Job Qualifications:
            Proven expertise in production software development
            7+ years of experience programming in Java, Python, SQL, or C/C++
            Proficient in SQL, NoSQL, relational database design and methods for efficiently retrieving data
            Strong analytical skills
            Creative problem solver
            Excellent verbal and written communications skills
            Strong team player capable of working in a demanding start-up environment
            Experience building complex and non-interactive systems (batch, distributed, etc.)
           Preferred Knowledge, Skills and Abilities:
            Prior consulting experience
            Experience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.
            Dynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)
            Experience designing and tuning high performance systems
            Prior experience with data warehousing and business intelligence systems
            Professional or academic background that includes mathematics, statistics, machine learning and data mining
            Linux expertise
            Prior work and/or research experience with unstructured data and data modeling
            Familiarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)
            Broader experience with spring ecosystem including spring-batch, spring-mvc, and spring-hadoop
            Standards-based REST implementation
            Configure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build
            Vagrant, Docker
            Familiar with OSI stack and proper use of HTTP verbs etc.
            Hive Tuning, Hive physical design (file formats, compression, partitioning, bucketing), Hive DSS Queries, Hive for Data Science, Hive for running R/Python (Streaming), Hive Transactions, Hive-HBase
            Knows how to tune a job including parameters, more efficient API calls. Understands Spark SWL. Understands Spark Streaming and can transform a DStream.
            Ability to write advanced UDFs, Serdes, input-loaders, log analysis, how the logical operators map to the lower, level physical implementation
            Setup and leverage output from Ganglia, Nagios, Ambari, Cloudera Manager, etc.
            Able to link Kerberos KDC to a backing LDAP or Active Directory authentication provider
            Advanced understanding of access-driven key design, appropriate denomalization, use of co-locating records of differing schemas in a single table etc.
            Understanding of best practices for Hive schemas. Denormalization, partitioning and bucketing, file formats.
            Able to create, write to and read from Kafka topic. Understanding of key partitioning (just how it works), able to maintain an offset in the topic for consistent reading
           Job Abilities:
          Must be able to sit for long periods of time working on computers. Must be able to travel to client sites at least 50% of the time. Must be able to interact and communicate with the client in meetings. Must be able to write programming code in applicable languages. Must be able to write project documentation in English.
           Education:
          Bachelor's Degree or foreign equivalent in Computer Science or related technical field followed by six (6) years of progressively responsible professional experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).
          OR
          Master's Degree or foreign equivalent in Computer Science or related technical field. Four (4) years of experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).
          Employer will accept any suitable combination of education, training, or experience.
           Think Big, A Teradata Company is an Equal Opportunity/Affirmative Action Employer and commits to hiring returning veterans.
           TB15
          :&#160;Services/Consulting
          :&#160;Full-time
          :&#160;Americas-United States-Georgia-Atlanta
          :&#160;AMS Sales &amp; Services
             Eclaro, Atlanta - GA
             Medical Neurogenetics, LLC, Atlanta - GA
             BridgeView IT, Atlanta - GA
             Elsym Consulting, Kennesaw - GA
             Anthem, Inc, Atlanta - GA
             AT&amp;T, Atlanta - GA
          Think Big Senior Data Engineer
        You will start receiving jobs like this emailed to: {emailaddress}