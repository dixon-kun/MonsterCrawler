('and', 46)('the', 26)('to', 22)('data', 20)('160', 19)('of', 19)('with', 19)('or', 12)('in', 11)('Data', 10)('solutions', 10)('8217', 9)('experience', 8)('Kylo', 8)('Big', 8)('-', 7)('on', 7)('&#', 6)('tools', 6)('Think', 6)('a', 6)('IL', 6)('Chicago', 6)('will', 6)('for', 6)('Experience', 6)(':&#', 5)('Senior', 5)('have', 5)('Hadoop', 5)('implementing', 5)('as', 5)('and/or', 5)('design', 4)('implement', 4)('such', 4)('Architect', 4)('They', 4)('implementation', 4)('including', 4)('both', 4)('s', 4)('You', 4)('Americas-United', 4)('using', 4)('security', 4)('an', 4)('Apache', 3)('understanding', 3)('technical', 3)('Matrix', 3)('customer', 3)('Spark', 3)('Solid', 3)('framework', 3)('our', 3)('open', 3)('The', 3)('enterprise', 3)('Resources', 3)('etc.)', 3)('testing', 3)('source', 3)('integration', 3)('be', 3)('helping', 3)('Lead', 2)('focus', 2)('Requirements:', 2)('environment', 2)('presentation', 2)('resources', 2)('team', 2)('requirements', 2)('what', 2)('designing', 2)('from', 2)('certified', 2)('work', 2)('CyberCoders', 2)('Integration', 2)('how', 2)('Hive', 2)('help', 2)('years', 2)('systems', 2)('good', 2)('network', 2)('re', 2)('Work', 2)('one', 2)('quality', 2)('management', 2)('needed', 2)('architect', 2)('Development', 2)('responsible', 2)('customer&#', 2)('performance', 2)('framework.&#', 2)('Principal', 2)('NiFi', 2)('distributed', 2)('teams', 2)('are', 2)('jobs', 2)('best', 2)('Cloudera', 2)('modeling', 2)('offerings', 2)('ETL', 2)('Degree', 2)('it', 2)('complex', 2)('flows', 2)('analysis', 2)('lake', 2)('customers', 2)('around', 2)('big', 2)('Java', 2)('scale', 2)('working', 2)('software', 2)('Teradata', 2)('Science', 2)('ecosystem', 2)('sales', 2)('you', 2)('building', 2)('all', 1)('Architect.&#', 1)('Responsibilities:', 1)('leads', 1)('States-North', 1)('NoSQL', 1)('Preferred', 1)('post-', 1)('returning', 1)('difference', 1)('HDFS', 1)('internally', 1)('Global', 1)('solution', 1)('Architect-', 1)('Headway', 1)('layer.', 1)('REST/JSON', 1)('XML/SOAP', 1)('+', 1)('transformed.&#', 1)('Carolina-Charlotte', 1)('metadata', 1)('supporting', 1)('concepts', 1)('(log', 1)('interpret', 1)('You&#', 1)('warehousing', 1)('larger', 1)('Staff', 1)('AWS', 1)('industry!', 1)('highly', 1)('Basic', 1)('use', 1)('network-based', 1)('consumption', 1)('Ranger', 1)('two', 1)('Solr', 1)('start', 1)('today', 1)('more', 1)('upcoming', 1)('started', 1)('about', 1)('aware', 1)('States-New', 1)('must', 1)('join', 1)('this', 1)('project.&#', 1)('setup', 1)('can', 1)('learn', 1)('following', 1)('for&#', 1)('scripting', 1)('IBM', 1)('process', 1)('monitoring', 1)('minimum', 1)('documenting', 1)('preferably', 1)('end', 1)('Kerberos', 1)('responsibilities', 1)('Significant', 1)('extensions', 1)('offshore', 1)('A', 1)('product', 1)('may', 1)('States-Illinois-Chicago', 1)('mapping', 1)('Specific', 1)('applications', 1)('produce', 1)('tier', 1)('effective', 1)('documentation', 1)('perform', 1)('Linux', 1)('Apply', 1)('ignite', 1)("Master's", 1)('Java/.NET', 1)('hiring', 1)('through', 1)('developer', 1)('bottleneck', 1)('writing', 1)('Director', 1)('Bachelor&#', 1)('production', 1)('(customer', 1)('MuleSoft', 1)('them', 1)('Installation', 1)('lineage', 1)('system.', 1)('|', 1)('End', 1)('Implementation', 1)('related', 1)('Perl', 1)('out', 1)('large', 1)('looking', 1)('Key', 1)('receiving', 1)('challenges.&#', 1)('Join', 1)('commits', 1)('GreenPlum/HAWQ', 1)('approaches', 1)('bash', 1)('onsite', 1)('training', 1)('logging', 1)('you&#', 1)('features', 1)('developing&#', 1)('engineer', 1)('APIs', 1)('leading', 1)('system', 1)('remotely', 1)('2', 1)('classroom', 1)('shell', 1)('Database', 1)('part', 1)('Enterprise', 1)('Oracle', 1)('than', 1)('to:', 1)('15', 1)('MongoDB', 1)('solutions.', 1)('Analytics', 1)('Unix', 1)('LDAP', 1)('Cassandra', 1)('debugging', 1)('Teradatas', 1)('Hortonworks', 1)('experience)', 1)('Custom', 1)('raw', 1)('(jconsole)', 1)('experience:', 1)('Do', 1)('(Kafka', 1)('experienced', 1)('PS', 1)('Equal', 1)('most', 1)('plan', 1)('scripts', 1)('engagements', 1)('QA', 1)('security.', 1)('senior', 1)('normally', 1)('Expert', 1)('consult', 1)('Recommend', 1)('staff', 1)('current', 1)('We', 1)('Infrastructure', 1)('knowledge', 1)('Network)', 1)('written', 1)('deployments', 1)('York', 1)('Services', 1)('Strong', 1)('Applied', 1)('Opportunity/Affirmative', 1)('get', 1)('overall', 1)('Keep', 1)('Products', 1)('Mentor', 1)('closely', 1)('optimally', 1)('eco-system', 1)('queries', 1)('pre-and', 1)('Storage', 1)('roadmap', 1)('profiling', 1)('project', 1)('Ability', 1)('(Kerberos/SPNEGO).', 1)('Tool', 1)('3', 1)('various', 1)('between', 1)('Full-time', 1)('terms', 1)('facing)', 1)('packages', 1)('taking', 1)('MapReduce', 1)('against', 1)('participating', 1)('Computer', 1)('Thinker?&#', 1)('large-scale', 1)('8211', 1)('Travel', 1)('/or', 1)('tuning', 1)('50%', 1)('.', 1)('interest', 1)('direction', 1)('needed.', 1)('options)', 1)('understand', 1)('Action', 1)('Professional', 1)('prove/show', 1)('Exadatas&#', 1)('(SQL', 1)('engineers', 1)('is', 1)('Write', 1)('Ambari', 1)('develop', 1)('Python', 1)('views', 1)('administration', 1)('Workforce', 1)('advise', 1)('veterans.', 1)('templates', 1)('170531', 1)('third-party', 1)('{emailaddress}', 1)('levels', 1)('architectures', 1)('(Hardware', 1)('center', 1)('Spring', 1)('Manager', 1)('practices', 1)('solve', 1)('storage', 1)('BI', 1)('JMX)', 1)('States-Georgia-Atlanta', 1)('world', 1)('Analyze', 1)('previous', 1)('discipline', 1)('processes.', 1)('Company', 1)('PIG', 1)('4', 1)('has', 1)('transformation', 1)('(Tibco', 1)('emailed', 1)('York-New', 1)('know', 1)('background', 1)('deployment', 1)('cluster-wide', 1)('projects', 1)('developers', 1)('like', 1)('success', 1)('specific', 1)('communications', 1)('enterprises', 1)('(or', 1)('SQL', 1)('become', 1)('passionate', 1)('often', 1)('verbal', 1)('people', 1)('some', 1)('Employer', 1)('Streaming', 1)('OSS', 1)('Big&#', 1)('leader', 1)('equivalent', 1)('processing', 1)('optimize', 1)('takes', 1)('pipeline', 1)('customers&#', 1)('ensure', 1)('Solutions', 1)('(HBase', 1)('consulting', 1)('own', 1)('Knowledge', 1)("world's", 1)('Storm', 1)('your', 1)('Netezzas', 1)('support', 1)('custom', 1)('doing', 1)('world&#', 1)('way', 1)('Services/Consulting', 1)('technologies', 1)('junior', 1)('j', 1)('up', 1)('Engineering', 1)('etc.', 1)('include:', 1)('globally', 1)('at', 1)('when', 1)('Delivery', 1)('setting', 1)('Understand', 1)('More', 1)('TB', 1)('EDW', 1)('Science.', 1)('Sentry', 1)('Drive', 1)('Americas', 1)('structure', 1)('lead', 1)('Hardware', 1)('Participate', 1)('Jobs.com', 1)    Think Big Senior Data Architect - Teradata | Jobs.com
          Think Big Senior Data Architect-170531
          Join Think Big Analytics, the world's leader in designing and building highly effective Big Data solutions. We are looking for an experienced engineer to become part of the Americas team, working with leading enterprises to design and implement solutions using our Kylo open source data lake framework.&#160;&#160;&#160;&#160; The Senior Data Architect will focus on implementation normally taking technical direction from the Principal Data Architect.&#160;&#160; They will consult with customers as needed both onsite and remotely and will be responsible for&#160; developing&#160; and documenting data lake solutions, helping to solve some of the world&#8217;s most complex big data challenges.&#160; They will own one or more data flows through one of our solutions, both in and out, raw and transformed.&#160;&#160; They have to have experience implementing data quality with a focus on lineage and data security. Drive overall implementation of supporting tools and processes. The senior has experience with data modeling and implementation of the mapping needed for the presentation layer. They must have a good understanding of best practices to structure the data optimally for consumption from an open source big data system.
          Key Responsibilities:
            The Senior Data Architect will be responsible for helping customers get started using our Kylo open source framework.&#160; Specific responsibilities may include:
            Implementation leads on a project.&#160; End to end data pipeline knowledge including metadata, security, data quality, data modeling, building custom views for applications and BI Tool presentation when needed.
            Understand how to implement with Kylo and NiFi and aware of upcoming features and roadmap to advise on specific implementation approaches
            Work with Principal architect and Delivery lead and with customer&#8217;s technical resources to implement solutions
            Lead team of Staff or offshore resources for larger scale projects and testing requirements
            Recommend and implement integration with third-party systems and network management tools
            Analyze complex distributed production deployments, and develop a plan to optimize performance
            Hardware and system software design, setup and testing
            Installation and testing of Kylo framework
            Development of data flows and templates, often working with the customer&#8217;s developers to help them learn how to use Kylo
            Development of Java extensions to the Kylo framework
            Custom integration of the Kylo framework to customer systems
            Write and produce technical documentation
            Lead or support Kylo developer training in a classroom setting
            Mentor junior engineers, staff and QA both with the customer and internally
            Work closely with Think Big&#8217; teams at all levels to help ensure the success of project consulting engagements with customer
            Participate in the pre-and post- sales process, helping both the sales and product teams to interpret customers&#8217; requirements
            Keep current with the Hadoop Big Data ecosystem technologies
            Travel up to 50%
          Requirements:
            You&#8217;re passionate about what you&#8217;re doing and ignite people around you
            You know your way around OSS and can prove/show it
            You are on good terms with Infrastructure (Hardware, Storage, Network), Java/.NET, and SQL . You have an interest in Data Science and understand the difference between Data Engineering and Applied Science.
            Master's Degree or Bachelor&#8217;s Degree in Computer Science or related discipline (or equivalent work experience) and the following minimum experience:
            More than two years of Professional Services (customer facing) experience implementing large scale storage, data center and /or globally distributed solutions
            2+ years participating in the design and deployment of 3 tier architectures or large-scale Hadoop solutions
            Solid verbal and written communications
            Experience with ETL solutions on Hadoop
            Experience implementing data transformation and processing solutions using Apache PIG, Hive (SQL on Hadoop options) or Spark
            Experience designing data queries against data in the HDFS environment using tools such as Apache Hive
            Experience implementing MapReduce and/or Spark jobs
            Experience with Ambari and/or Cloudera Enterprise Manager and Director
            Strong experience implementing software and/or solutions in the enterprise Linux or Unix environment
            Basic understanding with various enterprise security solutions such as LDAP and/or Kerberos, and Hadoop security packages such as Ranger or Sentry
            Expert with scripting tools such as bash shell scripts, Python and/or Perl
            Significant previous work writing to network-based APIs, preferably REST/JSON or XML/SOAP
            Ability to perform cluster-wide bottleneck analysis including network analysis and performance tuning
          Preferred Requirements:
            Experience with Apache NiFi
            Solid background in Database administration or design
            Knowledge of the data management eco-system including concepts of data warehousing, ETL, data integration, etc.
            Solid understanding of the Java ecosystem and enterprise offerings, including debugging and profiling tools (jconsole), logging and monitoring tools (log4j, JMX), and security offerings (Kerberos/SPNEGO).
            Streaming experience (Kafka, Storm, Spark, Solr, etc.)
            NoSQL experience (HBase, Cassandra, MongoDB, etc.)
            EDW experience &#8211; the Teradatas, Netezzas, GreenPlum/HAWQ, Exadatas&#8217; of the world
            Integration Products experience (Tibco, MuleSoft, IBM, Oracle, Spring Integration, etc.)
            Hortonworks or Cloudera certified
            AWS solution architect certified
          Do you have what it takes to be a Big Thinker?&#160; Apply today to join the best in the industry!
           Think Big, A Teradata Company is an Equal Opportunity/Affirmative Action Employer and commits to hiring returning veterans.
           TB15
          :&#160;Services/Consulting
          :&#160;Full-time
          :&#160;Americas-United States-Illinois-Chicago
          :&#160;Americas-United States-Georgia-Atlanta, Americas-United States-New York-New York, Americas-United States-North Carolina-Charlotte
          :&#160;Global PS
             CyberCoders, Chicago - IL
             CyberCoders, Chicago - IL
             Matrix Resources, Chicago - IL
             Headway Workforce Solutions, Chicago - IL
             Matrix Resources, Chicago - IL
             Matrix Resources, Chicago - IL
          Think Big Senior Data Architect
        You will start receiving jobs like this emailed to: {emailaddress}