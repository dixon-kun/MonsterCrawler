('with', 18)('and', 18)('of', 14)('Experience', 13)('-', 11)('data', 11)('to', 10)('MO', 10)('in', 10)('Hadoop', 9)('or', 8)('Data', 7)('Louis', 7)('St.', 6)('as', 6)('such', 6)('years', 5)('experience', 5)('are', 4)('on', 4)('2', 4)('a', 4)('the', 4)('Big', 3)('Maryland', 3)('degree', 3)('tools', 3)('from', 3)('Inc.', 3)('frameworks', 3)('Heights', 3)('Spectrum', 3)('solutions', 2)("Bachelor's", 2)('4', 2)('scalable', 2)('unstructured', 2)('using', 2)('processes', 2)('like', 2)('Pig', 2)('Sr.', 2)('Good', 2)('for', 2)('various', 2)('understanding', 2)('implementing', 2)('&#', 2)('Kafka', 2)('Design', 2)('relevant', 2)('ETL', 2)('working', 2)('systems', 2)('this', 2)('work', 2)('will', 2)('Engineer', 2)('Information', 2)('Description', 2)('Proficiency', 2)('databases', 2)('Hive', 2)('**', 2)('8226', 2)('Technology', 2)('required', 2)('NiFi', 1)('frameworks:', 1)('all', 1)('United', 1)('consider', 1)('Agile', 1)('meetings', 1)('move', 1)('QUALIFICATIONS', 1)('RabbitMQ', 1)('sources', 1)('Permanent', 1)('Apex', 1)('Saint', 1)('Group', 1)('Master&#', 1)('its', 1)('Resources', 1)('retention', 1)('1', 1)('Jobs', 1)('knowledge', 1)('Company', 1)('and&#', 1)('distributed', 1)('time.', 1)('capabilities', 1)('languages', 1)('according', 1)('Anthem', 1)('encouraged', 1)('8', 1)('accredited', 1)('Proficient', 1)('handle', 1)('Falcon', 1)('System.', 1)('big', 1)('Oozie', 1)('obtain', 1)('collaboratively', 1)('querying', 1)('every', 1)('relational', 1)('report', 1)('HBase', 1)('SparkML', 1)('HDFS', 1)('receiving', 1)('necessary', 1)('progressive', 1)('Tapestry', 1)('opportunity.', 1)('large', 1)('Inc', 1)('team', 1)('feeds', 1)('OS', 1)('|', 1)('Systems', 1)('Sprints', 1)('Neo', 1)('j', 1)('Java', 1)('Storm', 1)('SCALA', 1)('Citizen', 1)('principles', 1)('Platform', 1)('events/records', 1)('infrastructure', 1)('year', 1)('our', 1)('scrums', 1)('Scoop', 1)('Green', 1)('Ability', 1)('(HDP)', 1)('Selecting', 1)('integration', 1)('/', 1)('foreign', 1)('Job', 1)('unable', 1)('new', 1)('Zeplin.', 1)('Holder', 1)('U.S.', 1)('supporting', 1)('provide', 1)('jobs', 1)('business', 1)('This', 1)('s', 1)('equivalent', 1)('processing', 1)('We', 1)('O', 1)('job', 1)('advantages', 1)('Architecture', 1)('along', 1)('security', 1)('Linux', 1)('emailed', 1)('Matrix', 1)('launch', 1)('industry', 1)('programming', 1)('MapReduce', 1)('range', 1)('Solutions', 1)('160', 1)('messaging', 1)('Cassandra', 1)('changes', 1)('software', 1)('Monitoring', 1)('Knowledge', 1)('Partners', 1)('8217', 1)('three', 1)('Mahout', 1)('Preferred', 1)('cross-functional', 1)('lieu', 1)('administration', 1)('your', 1)('millions', 1)('Flume', 1)('management', 1)('Ranger', 1)('monthly', 1)('structured', 1)('Qualifications', 1)('transform', 1)('least', 1)('Implementing', 1)('authorized', 1)('6', 1)('start', 1)('Card', 1)('building', 1)('shell', 1)('pipelines', 1)('All', 1)('that', 1)('some', 1)('Citizens', 1)('specialty', 1)('scientists)', 1)('Drill', 1)('real-time', 1)('(SQL)', 1)('be', 1)('Lambda', 1)('to:', 1)('wide', 1)('Consulting', 1)('Sonoma', 1)('toolkits', 1)('integrating', 1)('ecosystem', 1)('reliable', 1)('Only', 1)('MongoDB', 1)('Full-Time', 1)('those', 1)('US', 1)('policies', 1)('v', 1)('following', 1)('Follow', 1)('scripting', 1)('Develop', 1)('Additional', 1)('computing', 1)('process', 1)('is', 1)('drawbacks', 1)('an', 1)('sponsor', 1)('advising', 1)('Ambari', 1)('You', 1)('ML', 1)('Defining', 1)('any', 1)('institution.', 1)('information', 1)('education', 1)('requirements', 1)('Phoenix', 1)('amounts', 1)('Marketing', 1)('Spark-Streaming', 1)('(account/product', 1)('who', 1)('also', 1)('build', 1)('Import/Export', 1)('performance', 1)('Spark', 1)('techniques', 1)('confidential', 1)('development', 1)('requested', 1)('Softpath', 1)('multiple', 1)('HCatalog', 1)('Will', 1)('applications', 1)('{emailaddress}', 1)('levels', 1)('NoSQL/Graph', 1)('apply.', 1)('independently', 1)('Python', 1)('kept', 1)('guidelines.', 1)('AVALA', 1)('Hortonworks', 1)('daily', 1)('States', 1)('efficient', 1)('ingestion', 1)('At', 1)('H', 1)('Participate', 1)('at', 1)('Jobs.com', 1)('stream-processing', 1)('EEO', 1)    Sr. Hadoop Data Engineer - Sonoma Consulting Inc. | Jobs.com
        Company Description
        Job Description
          Preferred
          Selecting and integrating Big Data tools and frameworks required to provide requested capabilities
          Implementing Data ingestion and ETL processes on Hadoop
          Monitoring performance and advising any necessary infrastructure changes
          Defining data retention policies
          Design and build data processing pipelines for structured and unstructured data using tools and frameworks in the Hadoop ecosystem
          Develop applications that are scalable to handle millions of events/records
          Design and launch scalable, reliable and efficient processes to move, transform and report on large amounts of data
          Participate in meetings with business (account/product management, data scientists) to obtain new requirements
          Follow our Agile software development process with daily scrums and monthly Sprints
          Ability to work collaboratively on a cross-functional team with a wide range of experience levels
          QUALIFICATIONS
          Bachelor's degree and&#160;8 years relevant experience or Master&#8217;s degree and 6 years of relevant experience
          4 years in industry implementing big data solutions on Hadoop
          Proficient understanding of distributed computing principles
          Proficiency with Hadoop v2, MapReduce, HDFS
          Experience with building stream-processing systems, using solutions such as Storm or Kafka and Spark-Streaming
          Good knowledge of Big Data querying tools, such as Pig, Hive, Phoenix
          Experience with Spark
          Experience with integration of data from multiple data sources
          Experience with 1 or 2 NoSQL/Graph databases, such as HBase, Cassandra, MongoDB, Neo4j
          Proficiency in a programming languages like SCALA, Java,Python
          Experience with Linux OS, shell scripting
          Experience with relational databases (SQL)
          Experience in working with real-time data feeds
          Experience in working with unstructured data
          Experience in implementing Scoop Jobs to Import/Export data from Hadoop
          Knowledge of various ETL techniques and frameworks, such as Pig, Hive, or Flume
          Experience with various messaging systems, such as Kafka or RabbitMQ
          Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O
          Good understanding of Lambda Architecture, along with its advantages and drawbacks
          Experience with Hortonworks Hadoop Data Platform (HDP)
          Experience with all or some of the following supporting Hadoop administration and security frameworks: HCatalog, Drill, NiFi, Oozie, Falcon, Ranger, Ambari, Zeplin.
        Qualifications
         &#8226;Bachelor's degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education
         &#8226;At least 2 years of experience with Information Technology
        Additional Information
         ** U.S. Citizens and those who are authorized to work independently in the United States are encouraged to apply. We are unable to sponsor at this time.
          This is a Full-Time / Permanent job opportunity.
          Only for US Citizen and Green Card Holder
         ** All your information will be kept confidential according to EEO guidelines.
             Anthem, Inc, Saint Louis - MO
             Spectrum, Maryland Heights - MO
             Spectrum, Maryland Heights - MO
             Spectrum, Maryland Heights - MO
             AVALA Marketing Group, St. Louis - MO
             Apex Systems, St. Louis - MO
             Tapestry Solutions Inc., St. Louis - MO
             Softpath System., St. Louis - MO
             Technology Partners, Inc., St. Louis - MO
             Matrix Resources, St. Louis - MO
          Sr. Hadoop Data Engineer
        You will start receiving jobs like this emailed to: {emailaddress}