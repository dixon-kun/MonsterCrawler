as a big data architect in the aero services organization you will be responsible for defining the aerospace-wide big data platform strategy that meet the need of our data science team and customer-facing web and mobile application this person will work closely with product management and engineering team to understand the customer requirement need aircraft system capability and available disparate data pipe to determine the optimal approach to managing load store access and govern the data in our big data platform
key to the growth of the aero services org will be our ability to monetize critical data that is produced by our suite of aerospace product
main responsibilities
understand and translate project requirement into technical requirement and solution for data engineering team to execute
architect and design a big data analytics platform that is scalable optimized and fault-tolerant
perform program review to ensure that data design element are reusable and repeatable across project
define and develop guideline standard and process to ensure the highest data quality and integrity in the data store residing on the data lake
participate in setting strategy and standard through data architecture and implementation leveraging big data and analytics tool and technology
work closely with data scientist and product manager to understand their data requirement for existing and future project on data analytics application
work with it and data owner to understand the type of data collected in various database and data warehouse
research and suggest new toolsets method to improve data ingestion storage and data access in the analytics platform
provide guidance mentor data engineering team
possess hands-on technical experience in big data technology
keen business acumen to recognize and recommend cost-effective and scalable platform solution that best meet current and future business need
ability to execute project using an agile approach in a multi-disciplinary matrixed environment
comfortable working in a dynamic research and development environment with several ongoing concurrent project
enjoys exploring and learning new technology
you must have
bachelor masters or phd degree in computer science it engineering or other relevant field with a 10 year of data management experience
minimum of 5 year of hands-on experience in designing deploying and supporting enterprise data warehouse and distributed data processing platform
minimum of 5 year of etl elt experience in traditional data mssql mysql oracle etc and big data platform using hadoop ecosystem spark hive pig sqoop flume etc
minimum of 3 year of experience in scripting language perl python java etc
minimum of 3 year of experience in nosql solution hbase cassandra mongodb couchdb etc and managing unstructured data
must be a us citizen due to contractual requirement
we value
certification in hadoop and other big data tool and technology
working experience in iot project
experience with data management on public cloud hosting service
deep knowledge in data mining machine learning natural language processing or information retrieval
experience with agile software development methodology
ability to work in a fast-paced and ambiguous environment
location
phoenix az