client overview
our client is utilizing cutting edge open source technology and framework to derive actionable insight from massive amount of data our solution provides capability to ingest petabyte of structured and unstructured data and build analytic pipeline that crunch through this data
job overview
we are looking for a big data engineer that will utilize our base platform to develop ingestion and analytic pipeline utilizing a multitude of open source technology the primary focus will be on implementing optimal solution utilizing best practice that can be shared across all our client this position requires working with technical lead data scientist and project manager in a scrum based agile environment
job responsibilities
create configure implement document and maintain ingestion enrichment and analytic pipeline using distributed big data platform
extend base platform functionality by adding new ingestion and analytic source
identify evaluate and implement big data tool and framework required to provide requested capability
job experience required
java development experience
scripting language experience perl python javascript
proficient understanding of distributed computing principle
proficiency with hadoop v2 mapreduce hdfs
experience with building stream-processing system using solution such a storm or spark
experience with integration of data from multiple data source
experience with nosql database such a elasticsearch mongodb cassandra
knowledge of various etl technique and framework such a flume logstash
experience with various messaging system such a kafka or rabbitmq
experience with cloudera hortonworks distribution
bachelor's degree or equivalent professional experience
job experience desired
good knowledge of big data querying tool such a pig hive and impala
management of hadoop cluster with all included service
ability to solve any ongoing issue with operating the cluster