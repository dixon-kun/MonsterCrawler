role purpose
the elsevier optimized learning suite eols is an adaptive learning project with the objective of improving the experience of learning by recommending customized content for it user through it usage of near real time streaming analytics the eols analytics platform utilizes a number of cutting edge technology in the hadoop ecosystem to perform stream processing batch analytics machine learning in the cloud these service developed will feed crucial business intelligence to our education data driven decision-making from both an institution and classroom perspective and provide analytical dashboard for both instructor and student with live data
as a senior hadoop engineer architect for the eols analytics platform you will have the following accountability
lead a team of highly motivated data integration engineer
provide technical advisory and expertise on analytics subject matter
create implement and execute the roadmap for providing analytics insight and machine learning
identify useful technology that can be used to fulfill user story requirement from an analytics perspective
experiment with new technology a an ongoing proof of concept
architect and develop data integration pipeline using a combination of stream and batch processing technique
integrate multiple data source using extraction transformation and loading etl
build data lake and data mart using hdfs nosql and relational database
manage multiple big data cluster and data storage in the cloud
collect and process event data from multiple application source with both internal elsevier and external vendor product
understand data science and work directly with data scientist and machine learning engineer
experience skills qualifications required
8 year experience in software programming using java javascript spring sql etc
3 year experience in service integration using rest soap rpc etc
3 year experience in data management data modeling
python scala or any semi-functional programming preferred
excellent sql skill from different range level of ansi compliancy
basic knowledge of applied statistics
advanced knowledge of systems and service architecture
advanced knowledge of polyglot persistence and use of rdbms in-memory key value store bigtable database and distributed file systems such a hdfs and amazon s3
industry experience working with large scale stream processing batch processing and data mining
extensive knowledge of the hadoop ecosystem and it component such a hdfs kafka spark flume oozie hbase hive
experience with at least one of the hadoop distribution such a cloudera hortonworks mapr or pivotal
experience with cloud service such a aws or azure
experience with linux unix system and the best practice for deploying application to hadoop from those environment
advanced knowledge of etl data routing and understanding of tool such a nifi kinesis etc
good understanding of devops sdlc and agile methodology
software infrastructure diagrams such a sequence uml data flows
requirements analysis planning problem solving strategic planning
excellent verbal communication self-motivated with initiative
education business domain knowledge preferred