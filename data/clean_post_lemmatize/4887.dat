job description
the biomedical informatics department at emory university is looking for a research
scientist in big data management and integration you will be responsible for
researching and developing data engineering solution to integrate and federate data
from various source of cancer research in the internet scale as a member of our
research group you will be expected to design implement and deploy distributed
system to manage data of heterogeneous nature storage and communication we are
looking for someone with a solid understanding and experience on distributed system
and cloud computing and is interested in learning and applying cutting edge technology
for the challenge of performance and scale the ideal candidate would be an
independent yet a team player with keen interest to collaborating and coordinating with
the team working on the data science research
this work is part of a large nih-funded project what this mean is that you will help
develop an exciting piece of technology that will be part of a national infrastructure
supporting cancer research one that brings together disparate cancer datasets and
give the research community the ability to examine data like never before your work
will have direct impact on cancer research and will become widely known through
national and international adoption collaborative participation in large-scale project and
open source software effort and most importantly in advancing healthcare
the ideal candidate is one who is motivated by challenging application and is interested
in exploring how advance in distributed computing and cloud computing can benefit
healthcare we collaborate extensively with well-known research group from prominent
institution and draw upon the first rate technical and scientific resource available at
emory and georgia tech
qualifications
you will be primarily developing server side application using java and a variety of
open-source big data framework such a apache drill and hadoop you will build upon
a suite of existing system that have been built in our lab and help add new functionality
and increase the scale of storage complexity of data and the variety of data type
required
phd in computer science or computer engineering with an emphasis on distributed
computing cloud computing hpc
expert in developing enterprise application with java with experience in concurrent
computing and stream
experienced in distributed system principle execution framework such a
hazelcast and infinispan and distributed storage and processing framework such
a apache hadoop
working knowledge of sql and nosql database
comfortable with public and private cloud deployment including the amazon web
service aws
experienced in web service engine and development of restful apis and web
service
experienced in various architecture and paradigm such a soa and osgi
unit and integration testing version control git and project management tool like
maven
track record of presenting at premier conference and publishing in relevant journal